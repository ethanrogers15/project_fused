{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from numpy.linalg import inv\n",
    "from mediapipe.tasks.python.components.containers import Detection, DetectionResult, BoundingBox, Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs:\n",
    "iou_threshold = 0.5\n",
    "decision_making_mode = 'all' # other two options could be 'thermal' and 'webcam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the number of images in the chosen test directory\n",
    "test_dir = Path('/project_fused/data/Testing_Gunner_and_Ethan_Walking')\n",
    "thermal_files = []\n",
    "for f in test_dir.joinpath('thermal').glob('*'):\n",
    "    thermal_files.append(f)\n",
    "thermal_list = sorted(thermal_files, key=lambda x: int(re.search(r'\\d+', x.stem).group()))\n",
    "num_images = len(thermal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the output directory\n",
    "output_dir = Path('/project_fused/output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738967110.004705   35046 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:77) display != EGL_NO_DISPLAYeglGetDisplay() returned error 0x300c\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1738967110.036905   35046 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:77) display != EGL_NO_DISPLAYeglGetDisplay() returned error 0x300c\n",
      "I0000 00:00:1738967110.099360   35046 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:77) display != EGL_NO_DISPLAYeglGetDisplay() returned error 0x300c\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Initialize the object detection models\n",
    "MAX_RESULTS = 2\n",
    "SCORE_THRESHOLD = 0 # In practical use, set higher, but for measuring AP, we want all detections\n",
    "\n",
    "base_options_webcam = python.BaseOptions(model_asset_path='/project_fused/models/efficientdet_lite0.tflite')\n",
    "options_webcam = vision.ObjectDetectorOptions(base_options=base_options_webcam, running_mode=vision.RunningMode.IMAGE, max_results=MAX_RESULTS, score_threshold=SCORE_THRESHOLD)\n",
    "webcam_detector = vision.ObjectDetector.create_from_options(options_webcam)\n",
    "\n",
    "# Initialize the thermal object detection model\n",
    "base_options_thermal = python.BaseOptions(model_asset_path='/project_fused/models/thermal.tflite')\n",
    "options_thermal = vision.ObjectDetectorOptions(base_options=base_options_thermal, running_mode=vision.RunningMode.IMAGE, max_results=MAX_RESULTS, score_threshold=SCORE_THRESHOLD)\n",
    "thermal_detector = vision.ObjectDetector.create_from_options(options_thermal)\n",
    "\n",
    "# Initialize the lidar object detection model\n",
    "base_options_lidar = python.BaseOptions(model_asset_path='/project_fused/models/lidar.tflite')\n",
    "options_lidar = vision.ObjectDetectorOptions(base_options=base_options_lidar, running_mode=vision.RunningMode.IMAGE, max_results=MAX_RESULTS, score_threshold=SCORE_THRESHOLD)\n",
    "lidar_detector = vision.ObjectDetector.create_from_options(options_lidar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define the transformation matrices\n",
    "# Set extrinsic translation matrices based on physical measurements, no z translation assumed\n",
    "T_l2t = array([[1, 0, 0, 0.028],\n",
    "                [0, 1, 0, -0.038],\n",
    "                [0, 0, 1, 0],\n",
    "                [0, 0, 0, 1]])\n",
    "T_l2w = array([[1, 0, 0, 0.083],\n",
    "                [0, 1, 0, -0.035],\n",
    "                [0, 0, 1, 0],\n",
    "                [0, 0, 0, 1]])\n",
    "\n",
    "# Set extrinsic rotation matrices from stereo calibration\n",
    "R_t2cₜ = array([[0.804905, 0.593319, 0.010014],\n",
    "                [-0.588094, 0.795337, 0.146920],\n",
    "                [0.079206, -0.124146, 0.989098]])\n",
    "R_l2cₜ = array([[0.813639, 0.571181, 0.108367],\n",
    "                [-0.580035, 0.784919, 0.217856],\n",
    "                [0.039376, -0.240112, 0.969946]])\n",
    "R_w2cᵣ = array([[0.903012, -0.397065, -0.164039],\n",
    "                [0.397183, 0.917127, -0.033513],\n",
    "                [0.163751, -0.034891, 0.985884]])\n",
    "R_l2cᵣ = array([[0.909488, -0.399788, -0.114025],\n",
    "                [0.399705, 0.916314, -0.024592],\n",
    "                [0.114314, -0.023211, 0.993173]])\n",
    "\n",
    "# Set intrinsic matrices for the three sensors\n",
    "Kₗ = array([[205.046875, 0.0, 107.55435943603516],\n",
    "            [0.0, 205.046875, 82.43924713134766],\n",
    "            [0.0, 0.0, 1.0]])\n",
    "Kₜ = array([[161.393925, 0.000000, 78.062273],\n",
    "            [0.000000, 161.761028, 59.925115], \n",
    "            [0.000000, 0.000000, 1.000000]])\n",
    "Kᵣ = array([[446.423112, 0.000000, 163.485603], \n",
    "            [0.000000, 446.765896, 131.217485],\n",
    "            [0.000000, 0.000000, 1.000000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: For loop through each set of synchronized images\n",
    "for i in range(num_images):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing, we will take one iteration from Step 5 and perform it here\n",
    "# Step 5a: Choose three synchronized images - need LiDAR equalized and 8 bit\n",
    "lidar_path = test_dir.joinpath('lidar/lidar_image_50.tiff')\n",
    "thermal_path = test_dir.joinpath('thermal/thermal_image_50.png')\n",
    "webcam_path = test_dir.joinpath('webcam/webcam_image_50.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5b: Read in the images as OpenCV images\n",
    "lidar_image = cv2.imread(lidar_path, cv2.IMREAD_UNCHANGED)\n",
    "thermal_image = cv2.imread(thermal_path, cv2.IMREAD_UNCHANGED)\n",
    "webcam_image = cv2.imread(webcam_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# Perform LiDAR image processing\n",
    "max_depth = np.max(lidar_image)\n",
    "lidar_image_clipped = np.clip(lidar_image, 0, max_depth)\n",
    "lidar_image_mm = lidar_image_clipped * 1000\n",
    "lidar_image_normalized = cv2.normalize(lidar_image_mm, None, 0, 65535, cv2.NORM_MINMAX)\n",
    "lidar_image_8bit = cv2.convertScaleAbs(lidar_image_normalized, alpha=(255.0 / np.max(lidar_image_normalized)))\n",
    "lidar_image_equalized = cv2.equalizeHist(lidar_image_8bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5c: Convert OpenCV images to RGB format\n",
    "lidar_image_rgb = cv2.cvtColor(lidar_image_equalized, cv2.COLOR_GRAY2RGB)\n",
    "thermal_image_rgb = cv2.cvtColor(thermal_image, cv2.COLOR_GRAY2RGB)\n",
    "webcam_image_rgb = cv2.cvtColor(webcam_image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5d: Convert RGB images to MediaPipe images\n",
    "lidar_image_mp = mp.Image(image_format=mp.ImageFormat.SRGB, data=lidar_image_rgb)\n",
    "thermal_image_mp = mp.Image(image_format=mp.ImageFormat.SRGB, data=thermal_image_rgb)\n",
    "webcam_image_mp = mp.Image(image_format=mp.ImageFormat.SRGB, data=webcam_image_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5e: Perform object detection on the MediaPipe images\n",
    "lidar_detection_result = lidar_detector.detect(lidar_image_mp)\n",
    "thermal_detection_result = thermal_detector.detect(thermal_image_mp)\n",
    "webcam_detection_result = webcam_detector.detect(webcam_image_mp)\n",
    "\n",
    "thermal_exclude_idx = []\n",
    "webcam_exclude_idx = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.929308295249939"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lidar_detection_result.detections[0].categories[0].score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5f: For loop through each LiDAR detection in the detection result\n",
    "for detection in lidar_detection_result.detections:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing, we will take one iteration from Step 5f and perform it here\n",
    "# Step 5f1: Define the top left and bottom right points of the detection\n",
    "lidar_detection = lidar_detection_result.detections[0]\n",
    "bbox = lidar_detection.bounding_box\n",
    "x1, y1 = bbox.origin_x, bbox.origin_y # Top left\n",
    "x2, y2 = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height # Bottom right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5f2: Find the depth on the LiDAR image at the center of the box\n",
    "uₗ = round((x1 + x2) / 2)\n",
    "vₗ = round((y1 + y2) / 2)\n",
    "zₗ = lidar_image[vₗ,uₗ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(zₗ, uₗ, vₗ):\n",
    "    if zₗ > 1E-8:\n",
    "        # Calculate the 3D physical coordinate of the center of the LiDAR image\n",
    "        pₗ = array([uₗ, vₗ, 1])\n",
    "        l̂ₗ = inv(Kₗ) @ pₗ\n",
    "        r̄ₗ = zₗ * l̂ₗ\n",
    "        \n",
    "        # Perform extrinsic translations to the thermal sensor and webcam\n",
    "        r̄ₜ = (inv(R_t2cₜ) @ (R_l2cₜ @ r̄ₗ)) + array([T_l2t[0, 3], T_l2t[1, 3], 0]).T\n",
    "        r̄ᵣ = (inv(R_w2cᵣ) @ (R_l2cᵣ @ r̄ₗ)) + array([T_l2w[0, 3], T_l2w[1, 3], 0]).T\n",
    "        \n",
    "        # Transform 3D coordinate to thermal and webcam pixel coordinates\n",
    "        r̃ₜ = array([r̄ₜ[0]/r̄ₜ[2], r̄ₜ[1]/r̄ₜ[2], r̄ₜ[2]/r̄ₜ[2]])\n",
    "        r̃ᵣ = array([r̄ᵣ[0]/r̄ᵣ[2], r̄ᵣ[1]/r̄ᵣ[2], r̄ᵣ[2]/r̄ᵣ[2]])\n",
    "        pₜ = Kₜ @ r̃ₜ\n",
    "        pᵣ = Kᵣ @ r̃ᵣ\n",
    "        uₜ, vₜ = pₜ[0], pₜ[1]\n",
    "        uᵣ, vᵣ = pᵣ[0], pᵣ[1]\n",
    "    \n",
    "    return uₜ, vₜ, uᵣ, vᵣ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5f3 & 5f4: If depth is not zero, then compute transformed u and v on webcam and thermal frames\n",
    "x1ₗₜ, y1ₗₜ, x1ₗᵣ, y1ₗᵣ = transform(zₗ, x1, y1)\n",
    "x2ₗₜ, y2ₗₜ, x2ₗᵣ, y2ₗᵣ = transform(zₗ, x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_iou(box_1, box_2):\n",
    "    # Get corner values from both boxes\n",
    "    x1, y1, x2, y2 = box_1\n",
    "    x3, y3, x4, y4 = box_2\n",
    "    \n",
    "    # Get corner values for the intersection box\n",
    "    x_inter1 = max(x1, x3)\n",
    "    y_inter1 = max(y1, y3)\n",
    "    x_inter2 = min(x2, x4)\n",
    "    y_inter2 = min(y2, y4)\n",
    "    \n",
    "    # Calculate the area of the intersection box\n",
    "    width_inter = x_inter2 - x_inter1\n",
    "    height_inter = y_inter2 - y_inter1\n",
    "    area_inter = width_inter * height_inter\n",
    "    \n",
    "    # Calculate the areas of the two boxes\n",
    "    width_box1 = x2 - x1\n",
    "    height_box1 = y2 - y1\n",
    "    width_box2 = x4 - x3\n",
    "    height_box2 = y4 - y3\n",
    "    area_box1 = width_box1 * height_box1\n",
    "    area_box2 = width_box2 * height_box2\n",
    "    \n",
    "    # Calculate the area of the full union of the two boxes\n",
    "    area_union = area_box1 + area_box2 - area_inter\n",
    "    \n",
    "    # Calculate the IoU\n",
    "    iou = area_inter / area_union\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5f5: Calculate IoU between the mapped bounding box and all detection results from the webcam and thermal images\n",
    "thermal_mapped_box = (x1ₗₜ, y1ₗₜ, x2ₗₜ, y2ₗₜ)\n",
    "thermal_ious = []\n",
    "for idxₜ, thermal_detection in enumerate(thermal_detection_result.detections):\n",
    "    if idxₜ in thermal_exclude_idx:\n",
    "        continue\n",
    "    thermal_bbox = thermal_detection.bounding_box\n",
    "    x1ₜ, y1ₜ = thermal_bbox.origin_x, thermal_bbox.origin_y\n",
    "    x2ₜ, y2ₜ = thermal_bbox.origin_x + thermal_bbox.width, thermal_bbox.origin_y + thermal_bbox.height\n",
    "    thermal_box = (x1ₜ, y1ₜ, x2ₜ, y2ₜ)\n",
    "    thermal_ious.append(calc_iou(thermal_box, thermal_mapped_box))\n",
    "\n",
    "webcam_mapped_box = (x1ₗᵣ, y1ₗᵣ, x2ₗᵣ, y2ₗᵣ)\n",
    "webcam_ious = []\n",
    "for idxᵣ, webcam_detection in enumerate(webcam_detection_result.detections):\n",
    "    if idxᵣ in webcam_exclude_idx:\n",
    "        continue\n",
    "    webcam_bbox = webcam_detection.bounding_box\n",
    "    x1ᵣ, y1ᵣ = webcam_bbox.origin_x, webcam_bbox.origin_y\n",
    "    x2ᵣ, y2ᵣ = webcam_bbox.origin_x + webcam_bbox.width, webcam_bbox.origin_y + webcam_bbox.height\n",
    "    webcam_box = (x1ᵣ, y1ᵣ, x2ᵣ, y2ᵣ)\n",
    "    webcam_ious.append(calc_iou(webcam_box, webcam_mapped_box))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5f6: Choose the thermal or webcam detection result corresponding to the LiDAR mapped result whose IoU is the \n",
    "#           largest and also above the defined Combination IoU threshold. In the next iterations of the for loop,\n",
    "#           the thermal or webcam detection result that was chosen should not be chosen again to match with another\n",
    "#           LiDAR mapped result\n",
    "max_thermal_iou = max(thermal_ious)\n",
    "max_thermal_iou_index = thermal_ious.index(max_thermal_iou)\n",
    "valid_thermal_iou = 0\n",
    "if max_thermal_iou > iou_threshold:\n",
    "    valid_thermal_iou, valid_thermal_idx = max_thermal_iou, max_thermal_iou_index\n",
    "    thermal_exclude_idx.append(valid_thermal_idx)\n",
    "    \n",
    "max_webcam_iou = max(webcam_ious)\n",
    "max_webcam_iou_index = webcam_ious.index(max_webcam_iou)\n",
    "valid_webcam_iou = 0\n",
    "if max_webcam_iou > iou_threshold:\n",
    "    valid_webcam_iou, valid_webcam_idx = max_webcam_iou, max_webcam_iou_index\n",
    "    webcam_exclude_idx.append(valid_webcam_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_detection(data):\n",
    "    bounding_box = BoundingBox(\n",
    "        origin_x=data[\"bounding_box\"][0],\n",
    "        origin_y=data[\"bounding_box\"][1],\n",
    "        width=data[\"bounding_box\"][2],\n",
    "        height=data[\"bounding_box\"][3]\n",
    "    )\n",
    "    \n",
    "    category = Category(\n",
    "        index=None,  # Optional\n",
    "        score=data[\"score\"],\n",
    "        display_name=None,  # Optional\n",
    "        category_name=data[\"category_name\"]\n",
    "    )\n",
    "    \n",
    "    detection = Detection(\n",
    "        bounding_box=bounding_box,\n",
    "        categories=[category],\n",
    "        keypoints=[]  # Optional\n",
    "    )\n",
    "\n",
    "    return detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_back(zₗ, uₜ, vₜ, uᵣ, vᵣ):\n",
    "    # Perform intrinsic transformations to get line of sight vectors\n",
    "    pₜ = array([uₜ, vₜ, 1])\n",
    "    l̂ₜ = inv(Kₜ) @ pₜ \n",
    "    pᵣ = array([uᵣ, vᵣ, 1])\n",
    "    l̂ᵣ = inv(Kᵣ) @ pᵣ\n",
    "    \n",
    "    # Add depth for position vectors\n",
    "    r̄ₜ = zₗ * l̂ₜ \n",
    "    r̄ᵣ = zₗ * l̂ᵣ\n",
    "    \n",
    "    # Perform extrinsic transformations to the LiDAR sensor\n",
    "    r̄ₗₜ = (inv(R_l2cₜ) @ (R_t2cₜ @ r̄ₜ)) - array([T_l2t[0, 3], T_l2t[1, 3], 0]).T\n",
    "    r̄ₗᵣ = (inv(R_l2cᵣ) @ (R_w2cᵣ @ r̄ᵣ)) - array([T_l2w[0, 3], T_l2w[1, 3], 0]).T\n",
    "    \n",
    "    # Transform to pixel coordinates\n",
    "    r̃ₗₜ = array([r̄ₗₜ[0]/r̄ₗₜ[2], r̄ₗₜ[1]/r̄ₗₜ[2], r̄ₗₜ[2]/r̄ₗₜ[2]])\n",
    "    r̃ₗᵣ = array([r̄ₗᵣ[0]/r̄ₗᵣ[2], r̄ₗᵣ[1]/r̄ₗᵣ[2], r̄ₗᵣ[2]/r̄ₗᵣ[2]])\n",
    "    pₗₜ = Kₗ @ r̃ₗₜ \n",
    "    pₗᵣ = Kₗ @ r̃ₗᵣ \n",
    "    uₗₜ, vₗₜ = pₗₜ[0], pₗₜ[1]\n",
    "    uₗᵣ, vₗᵣ = pₗᵣ[0], pₗᵣ[1]\n",
    "     \n",
    "    return uₗₜ, vₗₜ, uₗᵣ, vₗᵣ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_detections(lidar_detection, thermal_detection, webcam_detection, thermal_mapped_box, webcam_mapped_box):\n",
    "    # Get detection coordinates\n",
    "    thermal_bbox = thermal_detection.bounding_box\n",
    "    x1ₜ, y1ₜ = thermal_bbox.origin_x, thermal_bbox.origin_y\n",
    "    x2ₜ, y2ₜ = thermal_bbox.origin_x + thermal_bbox.width, thermal_bbox.origin_y + thermal_bbox.height\n",
    "    webcam_bbox = webcam_detection.bounding_box\n",
    "    x1ᵣ, y1ᵣ = webcam_bbox.origin_x, webcam_bbox.origin_y\n",
    "    x2ᵣ, y2ᵣ = webcam_bbox.origin_x + webcam_bbox.width, webcam_bbox.origin_y + webcam_bbox.height\n",
    "    x1ₗₜ, y1ₗₜ, x2ₗₜ, y2ₗₜ = thermal_mapped_box\n",
    "    x1ₗᵣ, y1ₗᵣ, x2ₗᵣ, y2ₗᵣ = webcam_mapped_box\n",
    "    \n",
    "    # Average coordinates between mapped and original thermal / webcam results\n",
    "    x1_avgₜ = (x1ₜ + x1ₗₜ) / 2\n",
    "    x2_avgₜ = (x2ₜ + x2ₗₜ) / 2\n",
    "    y1_avgₜ = (y1ₜ + y1ₗₜ) / 2\n",
    "    y2_avgₜ = (y2ₜ + y2ₗₜ) / 2\n",
    "    x1_avgᵣ = (x1ᵣ + x1ₗᵣ) / 2\n",
    "    x2_avgᵣ = (x2ᵣ + x2ₗᵣ) / 2\n",
    "    y1_avgᵣ = (y1ᵣ + y1ₗᵣ) / 2\n",
    "    y2_avgᵣ = (y2ᵣ + y2ₗᵣ) / 2\n",
    "    \n",
    "    # Average scores between mapped and original thermal / webcam / LiDAR results\n",
    "    thermal_avg_score = (lidar_detection.categories[0].score + thermal_detection.categories[0].score) / 2\n",
    "    webcam_avg_score = (lidar_detection.categories[0].score + webcam_detection.categories[0].score) / 2\n",
    "    lidar_avg_score = (lidar_detection.categories[0].score + thermal_detection.categories[0].score + webcam_detection.categories[0].score) / 3\n",
    "    \n",
    "    # Transform average results from thermal / webcam frames to the LiDAR frame\n",
    "    u1ₗₜ, v1ₗₜ, u1ₗᵣ, v1ₗᵣ = transform_back(zₗ, x1_avgₜ, y1_avgₜ, x1_avgᵣ, y1_avgᵣ)\n",
    "    u2ₗₜ, v2ₗₜ, u2ₗᵣ, v2ₗᵣ = transform_back(zₗ, x2_avgₜ, y2_avgₜ, x2_avgᵣ, y2_avgᵣ)\n",
    "    \n",
    "    # Average the two transformed average results to get the workflow result on the LiDAR frame\n",
    "    x1_avgₗ = (u1ₗₜ + u1ₗᵣ) / 2\n",
    "    x2_avgₗ = (u2ₗₜ + u2ₗᵣ) / 2\n",
    "    y1_avgₗ = (v1ₗₜ + v1ₗᵣ) / 2\n",
    "    y2_avgₗ = (v2ₗₜ + v2ₗᵣ) / 2\n",
    "    \n",
    "    # Create new detections for the averaged detections\n",
    "    avg_lidar_detection = {\n",
    "        \"bounding_box\": (x1_avgₗ, y1_avgₗ, x2_avgₗ - x1_avgₗ, y2_avgₗ - y1_avgₗ),\n",
    "        \"score\": lidar_avg_score,\n",
    "        \"category_name\": \"Person\"\n",
    "    }\n",
    "    avg_lidar_detection = create_detection(avg_lidar_detection)\n",
    "    \n",
    "    avg_thermal_detection = {\n",
    "        \"bounding_box\": (x1_avgₜ, y1_avgₜ, x2_avgₜ - x1_avgₜ, y2_avgₜ - y1_avgₜ),\n",
    "        \"score\": thermal_avg_score,\n",
    "        \"category_name\": \"Person\"\n",
    "    }\n",
    "    avg_thermal_detection = create_detection(avg_thermal_detection)\n",
    "    \n",
    "    avg_webcam_detection = {\n",
    "        \"bounding_box\": (x1_avgᵣ, y1_avgᵣ, x2_avgᵣ - x1_avgᵣ, y2_avgᵣ - y1_avgᵣ),\n",
    "        \"score\": webcam_avg_score,\n",
    "        \"category_name\": \"Person\"\n",
    "    }\n",
    "    avg_webcam_detection = create_detection(avg_webcam_detection)\n",
    "    \n",
    "    return avg_lidar_detection, avg_thermal_detection, avg_webcam_detection    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5f7: Depending on the Decision making mode, choose to either keep the mapped result or not based on whether there \n",
    "#           is agreement between all 3 or only two sensors\n",
    "# Step 5f8: If the mapped result is being kept, average the coordinates between the mapped result and corresponding\n",
    "#           individual thermal and webcam results. Then, transform the average result from the thermal and webcam frames\n",
    "#           back to the LiDAR frame. Next, average those two transformed average results to get the workflow result on the \n",
    "#           LiDAR frame. Now, there should be three averaged detection results corresponding to the three sensors. If the\n",
    "#           mapped result is not being kept, skip\n",
    "#NOTE: Add else: continue lines when inside the actual loop\n",
    "thermal_detection = thermal_detection_result.detections[valid_thermal_idx]\n",
    "webcam_detection = webcam_detection_result.detections[valid_webcam_idx]\n",
    "if decision_making_mode == 'all':\n",
    "    if valid_thermal_iou and valid_webcam_iou:\n",
    "        lidar_avg_detection, thermal_avg_detection, webcam_avg_detection = \\\n",
    "            average_detections(lidar_detection, thermal_detection, webcam_detection, thermal_mapped_box, webcam_mapped_box)\n",
    "\n",
    "if decision_making_mode == 'thermal':\n",
    "    if valid_thermal_iou:\n",
    "        lidar_avg_detection, thermal_avg_detection, webcam_avg_detection = \\\n",
    "            average_detections(lidar_detection, thermal_detection, webcam_detection, thermal_mapped_box, webcam_mapped_box)\n",
    "\n",
    "if decision_making_mode == 'webcam':\n",
    "    if valid_webcam_iou:\n",
    "        lidar_avg_detection, thermal_avg_detection, webcam_avg_detection = \\\n",
    "            average_detections(lidar_detection, thermal_detection, webcam_detection, thermal_mapped_box, webcam_mapped_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5f9: Store the three averaged detection results at each iteration\n",
    "# WHEN THE ACTUAL LOOP IS FORMED, ADD CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_COLOR = (0, 0, 255)\n",
    "BOX_THICKNESS = 3\n",
    "MARGIN = 5\n",
    "ROW_SIZE = -15\n",
    "FONT_SIZE = 0.5\n",
    "FONT_THICKNESS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image, detection_result):\n",
    "\n",
    "    for detection in detection_result.detections:\n",
    "        # Draw the bounding box.\n",
    "        bbox = detection.bounding_box\n",
    "        start_point = bbox.origin_x, bbox.origin_y\n",
    "        end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
    "        cv2.rectangle(image, start_point, end_point, TEXT_COLOR, BOX_THICKNESS)\n",
    "\n",
    "        # Write the label.\n",
    "        category = detection.categories[0]\n",
    "        category_name = category.category_name\n",
    "        probability = round(category.score, 2)\n",
    "        result_text = category_name + ' (' + str(probability) + ')'\n",
    "        text_location = (MARGIN + bbox.origin_x,\n",
    "                            MARGIN + ROW_SIZE + bbox.origin_y)\n",
    "        cv2.putText(image, result_text, text_location, cv2.FONT_HERSHEY_DUPLEX,\n",
    "                    FONT_SIZE, TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5g: With all of the averaged, mapped detection results, and with all of the individual sensor detection results, draw\n",
    "#          the bounding boxes on the images corresponding to the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5h: Output the 3 new images (LiDAR, thermal, webcam) with bounding boxes drawn on them to the output directory for viewing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
